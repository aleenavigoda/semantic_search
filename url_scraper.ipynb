{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_urls():\n",
    "    # The correct URL structure based on the screenshot\n",
    "    base_url = \"https://worksinprogress.co/issue-{}\"\n",
    "    return [base_url.format(i) for i in range(1, 17)]  # Issues 1 to 16\n",
    "\n",
    "def get_essay_urls_from_issue(issue_url):\n",
    "    response = requests.get(issue_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using the correct selector based on the screenshot\n",
    "    article_links = soup.select('article.article-card header.article-card__head a')\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in article_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://worksinprogress.co' + url\n",
    "        essay_urls.append(url)\n",
    "    \n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    all_essay_urls = []\n",
    "    issue_urls = get_issue_urls()\n",
    "    \n",
    "    for issue_url in issue_urls:\n",
    "        print(f\"Scraping essays from {issue_url}\")\n",
    "        essay_urls = get_essay_urls_from_issue(issue_url)\n",
    "        all_essay_urls.extend(essay_urls)\n",
    "        time.sleep(1)  # Be respectful with rate limiting\n",
    "    \n",
    "    return all_essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Optionally, save URLs to a file\n",
    "    with open('work_in_progress_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_archive_url():\n",
    "    return \"https://www.worksinprogress.news/archive\"\n",
    "\n",
    "def get_essay_urls_from_archive(archive_url):\n",
    "    response = requests.get(archive_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using the correct selector based on the screenshot\n",
    "    article_links = soup.select('a[data-testid=\"post-preview-title\"]')\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in article_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://www.worksinprogress.news' + url\n",
    "        essay_urls.append(url)\n",
    "    \n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    archive_url = get_archive_url()\n",
    "    print(f\"Scraping essays from {archive_url}\")\n",
    "    essay_urls = get_essay_urls_from_archive(archive_url)\n",
    "    return essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Optionally, save URLs to a file\n",
    "    with open('works_in_progress_substack_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_archive_url():\n",
    "    return \"https://www.worksinprogress.news/archive\"\n",
    "\n",
    "def scroll_to_bottom(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def get_essay_urls_from_archive(archive_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    driver.get(archive_url)\n",
    "    \n",
    "    # Scroll to load all articles\n",
    "    scroll_to_bottom(driver)\n",
    "    \n",
    "    # Wait for articles to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'a[data-testid=\"post-preview-title\"]'))\n",
    "    )\n",
    "    \n",
    "    # Get the page source after scrolling\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all article links\n",
    "    article_links = soup.select('a[data-testid=\"post-preview-title\"]')\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in article_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://www.worksinprogress.news' + url\n",
    "        essay_urls.append(url)\n",
    "    \n",
    "    driver.quit()\n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    archive_url = get_archive_url()\n",
    "    print(f\"Scraping essays from {archive_url}\")\n",
    "    essay_urls = get_essay_urls_from_archive(archive_url)\n",
    "    return essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Save URLs to a file\n",
    "    with open('works_in_progress_substack_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_issue_urls():\n",
    "    base_url = \"https://worksinprogress.co/issue-{}\"\n",
    "    return [base_url.format(i) for i in range(1, 17)]  # Issues 1 to 16\n",
    "\n",
    "def get_essay_urls_from_issue(issue_url):\n",
    "    response = requests.get(issue_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Correct selectors based on the HTML structure we confirmed earlier\n",
    "    spotlight_links = soup.select('article.spotlight-card .spotlight-card_title > a')\n",
    "    grid_links = soup.select('article.article-card header.article-card_head > a')\n",
    "    \n",
    "    all_links = spotlight_links + grid_links\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in all_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://worksinprogress.co' + url\n",
    "        essay_urls.append(url)\n",
    "    \n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    all_essay_urls = []\n",
    "    issue_urls = get_issue_urls()\n",
    "    \n",
    "    for issue_url in issue_urls:\n",
    "        print(f\"Scraping essays from {issue_url}\")\n",
    "        essay_urls = get_essay_urls_from_issue(issue_url)\n",
    "        all_essay_urls.extend(essay_urls)\n",
    "        time.sleep(1)  # Be respectful with rate limiting\n",
    "    \n",
    "    return all_essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Optionally, save URLs to a file\n",
    "    with open('work_in_progress_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_issue_urls():\n",
    "    base_url = \"https://worksinprogress.co/issue-{}\"\n",
    "    return [base_url.format(i) for i in range(1, 17)]  # Issues 1 to 16\n",
    "\n",
    "def get_essay_urls_from_issue(issue_url):\n",
    "    print(f\"Fetching {issue_url}\")\n",
    "    response = requests.get(issue_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Received status code {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using the correct selector format that we know worked\n",
    "    grid_links = soup.select('article.article-card header.article-card__head a')\n",
    "    \n",
    "    # Adding spotlight articles with a similar structure\n",
    "    spotlight_links = soup.select('article.spotlight-card .spotlight-card__title a')\n",
    "    \n",
    "    all_links = spotlight_links + grid_links\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in all_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://worksinprogress.co' + url\n",
    "        if url not in essay_urls:  # Avoid duplicates\n",
    "            essay_urls.append(url)\n",
    "    \n",
    "    print(f\"Found {len(essay_urls)} unique essay URLs in this issue\")\n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    all_essay_urls = []\n",
    "    issue_urls = get_issue_urls()\n",
    "    \n",
    "    for issue_url in issue_urls:\n",
    "        print(f\"\\nScraping essays from {issue_url}\")\n",
    "        essay_urls = get_essay_urls_from_issue(issue_url)\n",
    "        all_essay_urls.extend(essay_urls)\n",
    "        time.sleep(1)  # Be respectful with rate limiting\n",
    "    \n",
    "    return all_essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Save URLs to a file\n",
    "    with open('work_in_progress_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_url(url):\n",
    "    print(f\"Attempting to access: {url}\")\n",
    "    response = requests.get(url)\n",
    "    print(f\"Status code: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        print(\"\\nFirst 1000 characters of the page content:\")\n",
    "        print(soup.prettify()[:1000])\n",
    "        \n",
    "        print(\"\\nAll 'article' tags found:\")\n",
    "        articles = soup.find_all('article')\n",
    "        for i, article in enumerate(articles, 1):\n",
    "            print(f\"\\nArticle {i}:\")\n",
    "            print(article.prettify()[:500])  # Print first 500 characters of each article\n",
    "    else:\n",
    "        print(\"Failed to access the page. Please check the URL.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://worksinprogress.co\"\n",
    "    check_url(base_url)\n",
    "    \n",
    "    issue_url = \"https://worksinprogress.co/issue-1\"\n",
    "    check_url(issue_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_issue_urls():\n",
    "    base_url = \"https://worksinprogress.co/issue-{}\"\n",
    "    return [base_url.format(i) for i in range(1, 17)]  # Issues 1 to 16\n",
    "\n",
    "def get_essay_urls_from_issue(issue_url):\n",
    "    print(f\"Scraping essays from {issue_url}\")\n",
    "    response = requests.get(issue_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using the correct selector for regular articles\n",
    "    regular_links = soup.select('article.article-card header.article-card__head a')\n",
    "    \n",
    "    # Adding a selector for spotlight articles\n",
    "    spotlight_links = soup.select('article.spotlight-card a.spotlight-card__link')\n",
    "    \n",
    "    all_links = regular_links + spotlight_links\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in all_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://worksinprogress.co' + url\n",
    "        if url not in essay_urls:  # Avoid duplicates\n",
    "            essay_urls.append(url)\n",
    "    \n",
    "    print(f\"Found {len(essay_urls)} unique essay URLs in this issue\")\n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    all_essay_urls = []\n",
    "    issue_urls = get_issue_urls()\n",
    "    \n",
    "    for issue_url in issue_urls:\n",
    "        essay_urls = get_essay_urls_from_issue(issue_url)\n",
    "        all_essay_urls.extend(essay_urls)\n",
    "        time.sleep(1)  # Be respectful with rate limiting\n",
    "    \n",
    "    return all_essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Save URLs to a file\n",
    "    with open('work_in_progress_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_issue_urls():\n",
    "    base_url = \"https://worksinprogress.co/issue-{}\"\n",
    "    return [base_url.format(i) for i in range(1, 17)]  # Issues 1 to 16\n",
    "\n",
    "def get_essay_urls_from_issue(issue_url):\n",
    "    print(f\"Scraping essays from {issue_url}\")\n",
    "    response = requests.get(issue_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using the correct selector for regular articles\n",
    "    all_links = soup.select('article.article-card header.article-card__head a, article.spotlight-card .spotlight-card__title a')\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in all_links:\n",
    "        url = link['href']\n",
    "        # Exclude author links and ensure we're only getting article links\n",
    "        if '/issue/' in url and 'our-authors' not in url:\n",
    "            if not url.startswith('http'):\n",
    "                url = 'https://worksinprogress.co' + url\n",
    "            if url not in essay_urls:  # Avoid duplicates\n",
    "                essay_urls.append(url)\n",
    "    \n",
    "    print(f\"Found {len(essay_urls)} unique essay URLs in this issue\")\n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    all_essay_urls = []\n",
    "    issue_urls = get_issue_urls()\n",
    "    \n",
    "    for issue_url in issue_urls:\n",
    "        essay_urls = get_essay_urls_from_issue(issue_url)\n",
    "        all_essay_urls.extend(essay_urls)\n",
    "        time.sleep(1)  # Be respectful with rate limiting\n",
    "    \n",
    "    return all_essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Save URLs to a file\n",
    "    with open('work_in_progress_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def get_archive_url():\n",
    "    return \"https://instituteforprogress.substack.com/archive\"\n",
    "\n",
    "def get_essay_urls_from_archive(archive_url):\n",
    "    response = requests.get(archive_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Using the correct selector based on the screenshot\n",
    "    article_links = soup.select('a[data-testid=\"post-preview-title\"]')\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in article_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://instituteforprogress.substack.com' + url\n",
    "        essay_urls.append(url)\n",
    "    \n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    archive_url = get_archive_url()\n",
    "    print(f\"Scraping essays from {archive_url}\")\n",
    "    essay_urls = get_essay_urls_from_archive(archive_url)\n",
    "    return essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Optionally, save URLs to a file\n",
    "    with open('institute_for_progress_substack_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping essays from https://nathanpmyoung.substack.com/archive\n",
      "\n",
      "Total essays found: 69\n",
      "https://nathanpmyoung.substack.com/p/pantheon-is-really-good\n",
      "https://nathanpmyoung.substack.com/cp/147682155\n",
      "https://nathanpmyoung.substack.com/p/truth-seeking-projects-im-interested\n",
      "https://nathanpmyoung.substack.com/p/forecasting-is-mostly-vibes-so-is\n",
      "https://nathanpmyoung.substack.com/p/ai-and-integrity\n",
      "https://nathanpmyoung.substack.com/p/questions-are-too-cheap\n",
      "https://nathanpmyoung.substack.com/cp/144402757\n",
      "https://nathanpmyoung.substack.com/p/advice-for-high-schoolers\n",
      "https://nathanpmyoung.substack.com/p/that-carlsmith-blog-series-in-1-page\n",
      "https://nathanpmyoung.substack.com/p/my-understanding-of-truth\n",
      "https://nathanpmyoung.substack.com/p/lets-fix-project-overruns\n",
      "https://nathanpmyoung.substack.com/p/up-for-debate\n",
      "https://nathanpmyoung.substack.com/p/theseus-and-the-minotaur\n",
      "https://nathanpmyoung.substack.com/p/5-of-a-kind\n",
      "https://nathanpmyoung.substack.com/p/be-more-katja\n",
      "https://nathanpmyoung.substack.com/p/grief-is-a-fire-sale\n",
      "https://nathanpmyoung.substack.com/p/the-world-in-2029\n",
      "https://nathanpmyoung.substack.com/p/boring-nihilism\n",
      "https://nathanpmyoung.substack.com/p/how-not-to-go-insane\n",
      "https://nathanpmyoung.substack.com/p/an-effective-altruists-guide-to-the\n",
      "https://nathanpmyoung.substack.com/p/uk-election-dashboard-bad-prototype\n",
      "https://nathanpmyoung.substack.com/p/introducing-viewpointsxyz\n",
      "https://nathanpmyoung.substack.com/p/minimum-viable-paradise\n",
      "https://nathanpmyoung.substack.com/p/ai-probability-trees-joe-carlsmith\n",
      "https://nathanpmyoung.substack.com/p/you-have-an-oracle-now-what\n",
      "https://nathanpmyoung.substack.com/p/greenbelt-vignettes\n",
      "https://nathanpmyoung.substack.com/p/liberal-christianity-music-festivals\n",
      "https://nathanpmyoung.substack.com/p/forecasting-norms-and-the-island\n",
      "https://nathanpmyoung.substack.com/p/what-might-happen-with-ai-katja-grace\n",
      "https://nathanpmyoung.substack.com/p/artificial-intelligence-riskreward\n",
      "https://nathanpmyoung.substack.com/p/on-loyalty\n",
      "https://nathanpmyoung.substack.com/p/its-okay-to-leave\n",
      "https://nathanpmyoung.substack.com/p/coming-soon\n",
      "https://nathanpmyoung.substack.com/p/11-dos-and-dont-for-weddings-596f7b5fc645\n",
      "https://nathanpmyoung.substack.com/p/the-good-place-all-good-things-must-come-to-an-end-5fe5c641173f\n",
      "https://nathanpmyoung.substack.com/p/the-striped-herd-3c9aece2167\n",
      "https://nathanpmyoung.substack.com/p/the-prospector-19621d262587\n",
      "https://nathanpmyoung.substack.com/p/why-did-you-leave-christianity-1bf82651c900\n",
      "https://nathanpmyoung.substack.com/p/i-went-to-a-really-good-conference-run-on-grip-recently-9d4ede8cf722\n",
      "https://nathanpmyoung.substack.com/p/its-not-what-you-do-it-s-what-you-do-first-5c153142c6fe\n",
      "https://nathanpmyoung.substack.com/p/win-win-not-all-games-need-a-loser-7ceac56f1d0a\n",
      "https://nathanpmyoung.substack.com/p/do-you-have-the-data-in-the-graph-above-about-which-countries-have-taken-which-responses-d366903e06cd\n",
      "https://nathanpmyoung.substack.com/p/the-hitchhikers-guide-to-coronavirus-27008ec07325\n",
      "https://nathanpmyoung.substack.com/p/thanks-for-writing-this-and-your-thougths-1a810c51f092\n",
      "https://nathanpmyoung.substack.com/p/the-life-of-pi-choosing-between-stories-ba3d322a9db6\n",
      "https://nathanpmyoung.substack.com/p/who-are-parlaiments-rivals-that-you-are-thinking-of-here-bf599d5fbdf2\n",
      "https://nathanpmyoung.substack.com/p/its-no-surprise-the-iowa-caucus-was-a-trainwreck-67880fb287a9\n",
      "https://nathanpmyoung.substack.com/p/its-okay-i-have-conservative-friends-d8a49c289369\n",
      "https://nathanpmyoung.substack.com/p/netflix-and-venerate-ce1445c9a6cd\n",
      "https://nathanpmyoung.substack.com/p/is-this-thing-on-e5512d74b89c\n",
      "https://nathanpmyoung.substack.com/p/a-cafe-on-the-seafront-half-empty-9af940cc762a\n",
      "https://nathanpmyoung.substack.com/p/youngs-first-law-of-worldbuilding-rules-and-form-4e9174ae3645\n",
      "https://nathanpmyoung.substack.com/p/stopping-not-starting-9e2c532e1c14\n",
      "https://nathanpmyoung.substack.com/p/the-guardians-farron-story-is-disconcertingly-poor-journalism-722cd0c216e0\n",
      "https://nathanpmyoung.substack.com/p/the-westminster-attack-was-a-tragedy-how-should-we-feel-about-it-42bba8fb0afe\n",
      "https://nathanpmyoung.substack.com/p/some-people-will-believe-what-they-want-to-believe-8ea70c803215\n",
      "https://nathanpmyoung.substack.com/p/easter-sunday-what-do-you-think-happened-9ddfdf3a3a68\n",
      "https://nathanpmyoung.substack.com/p/some-proposals-from-a-rational-christian-point-of-view-f2f21f79a8ea\n",
      "https://nathanpmyoung.substack.com/p/street-harassment-4e9344692629\n",
      "https://nathanpmyoung.substack.com/p/star-wars-ep-9-spoilers-701a1dc796ee\n",
      "https://nathanpmyoung.substack.com/p/26-thoughts-on-avengers-infinity-war-spoilers-7b9cdb254aea\n",
      "https://nathanpmyoung.substack.com/p/the-rise-and-rise-of-2048-c1dbe41c6f3\n",
      "https://nathanpmyoung.substack.com/p/people-who-wander-in-circles-departure-day-4ada8bfb6573\n",
      "https://nathanpmyoung.substack.com/p/i-dont-love-clifford-the-big-red-dog-b5b5313a253\n",
      "https://nathanpmyoung.substack.com/p/if-i-were-a-tory-c8652ef0d140\n",
      "https://nathanpmyoung.substack.com/p/how-does-endgame-work-massive-spoilers-4bda64e17891\n",
      "https://nathanpmyoung.substack.com/p/rui-i-hope-youve-figured-something-out-and-maybe-this-will-not-help-you-but-i-think-the-change-b6e36cb17601\n",
      "https://nathanpmyoung.substack.com/p/hi-i-have-the-same-problem-6eb3fb9df20c\n",
      "https://nathanpmyoung.substack.com/p/thanks-for-a-thought-provoking-article-a21c3ac05460\n"
     ]
    }
   ],
   "source": [
    "def get_archive_url():\n",
    "    return \"https://nathanpmyoung.substack.com/archive\"\n",
    "\n",
    "def scroll_to_bottom(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for page to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def get_essay_urls_from_archive(archive_url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    \n",
    "    driver.get(archive_url)\n",
    "    \n",
    "    # Scroll to load all articles\n",
    "    scroll_to_bottom(driver)\n",
    "    \n",
    "    # Wait for articles to load\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, 'a[data-testid=\"post-preview-title\"]'))\n",
    "    )\n",
    "    \n",
    "    # Get the page source after scrolling\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Find all article links\n",
    "    article_links = soup.select('a[data-testid=\"post-preview-title\"]')\n",
    "    \n",
    "    essay_urls = []\n",
    "    for link in article_links:\n",
    "        url = link['href']\n",
    "        if not url.startswith('http'):\n",
    "            url = 'https://nathanpmyoung.substack.com' + url\n",
    "        essay_urls.append(url)\n",
    "    \n",
    "    driver.quit()\n",
    "    return essay_urls\n",
    "\n",
    "def scrape_all_essay_urls():\n",
    "    archive_url = get_archive_url()\n",
    "    print(f\"Scraping essays from {archive_url}\")\n",
    "    essay_urls = get_essay_urls_from_archive(archive_url)\n",
    "    return essay_urls\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    essay_urls = scrape_all_essay_urls()\n",
    "    \n",
    "    print(f\"\\nTotal essays found: {len(essay_urls)}\")\n",
    "    for url in essay_urls:\n",
    "        print(url)\n",
    "\n",
    "    # Save URLs to a file\n",
    "    with open('predictive_text_substack_urls.txt', 'w') as f:\n",
    "        for url in essay_urls:\n",
    "            f.write(f\"{url}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
