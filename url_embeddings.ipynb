{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install supabase pandas sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleenavigoda/search_embeddings/.conda/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from supabase import create_client, Client\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Connect to Supabase\n",
    "SUPABASE_URL = \"https://hyxoojvfuuvjcukjohyi.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Imh5eG9vanZmdXV2amN1a2pvaHlpIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MjgzMTU4ODMsImV4cCI6MjA0Mzg5MTg4M30.eBQ3JLM9ddCmPeVq_cMIE4qmm9hqr_HaSwR88wDK8w0\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch URLs from Supabase table\n",
    "data = supabase.table('urls_table').select('id', 'url').execute()\n",
    "urls = [row['url'] for row in data.data]\n",
    "ids = [row['id'] for row in data.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleenavigoda/search_embeddings/.conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 32/32 [00:05<00:00,  6.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load MiniLM model and generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(urls, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Supabase with generated embeddings\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    embedding_list = embedding.tolist()  # Convert NumPy array to list\n",
    "    supabase.table('urls_table').update({'url_embedding': embedding_list}).eq('id', ids[i]).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs to retry: 99\n"
     ]
    }
   ],
   "source": [
    "# Fetch rows with NULL embeddings (in this case, 1001-1099)\n",
    "# Fetch all rows with NULL embeddings\n",
    "data = supabase.table('urls_table').select('id', 'url').is_('url_embedding', None).execute()\n",
    "\n",
    "# Extract URLs and IDs for reprocessing\n",
    "urls_to_retry = [row['url'] for row in data.data]\n",
    "ids_to_retry = [row['id'] for row in data.data]\n",
    "print(f\"Number of URLs to retry: {len(urls_to_retry)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  5.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for the URLs that need reprocessing\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the MiniLM model if it's not already loaded\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings for the URLs to retry\n",
    "embeddings_to_retry = model.encode(urls_to_retry, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the embeddings for the specific rows that were missed\n",
    "for i, embedding in enumerate(embeddings_to_retry):\n",
    "    embedding_list = embedding.tolist()  # Convert NumPy array to list\n",
    "    supabase.table('urls_table').update({'url_embedding': embedding_list}).eq('id', ids_to_retry[i]).execute()\n",
    "    print(f\"Uploaded embedding for ID: {ids_to_retry[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply t-SNE to reduce dimensionality to 2D for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_to_retry)\n",
    "\n",
    "# Plotting the 2D representation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "plt.title('t-SNE Visualization of URL Embeddings')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai scikit-learn numpy sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"Ysk-proj-hdQjtereUKOUwV0xRE8E8q6fbWj3VUmzl4hp7orKXYJls_UwQa80NCu5Ol04T_KWzYiMRZJBvlT3BlbkFJ1TvYwqBRGd4Rr5vX--EKEfWe9LfgHXugEf0VAe2HrKUZK1WBezJZBIU5uRknQtQy7oC8tHSbkA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an embedding for the user's prompt\n",
    "def get_prompt_embedding(prompt):\n",
    "    response = openai.Embedding.create(\n",
    "        input=prompt,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = \"Ysk-proj-hdQjtereUKOUwV0xRE8E8q6fbWj3VUmzl4hp7orKXYJls_UwQa80NCu5Ol04T_KWzYiMRZJBvlT3BlbkFJ1TvYwqBRGd4Rr5vX--EKEfWe9LfgHXugEf0VAe2HrKUZK1WBezJZBIU5uRknQtQy7oC8tHSbkA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an embedding for the user's prompt using the updated API call\n",
    "def get_prompt_embedding(prompt):\n",
    "    response = openai.Embedding.create(\n",
    "        model=\"text-embedding-ada-002\",\n",
    "        input=[prompt]  # Note: input must be a list of strings in newer versions\n",
    "    )\n",
    "    return response['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleenavigoda/search_embeddings/.conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the MiniLM model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt embedding generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# User input\n",
    "prompt = \"I want to read about speculative fiction in the context of technology and its impact on society.\"\n",
    "\n",
    "# Get the embedding for the user prompt\n",
    "prompt_embedding = model.encode([prompt])  # Input needs to be a list, even if it's just one prompt\n",
    "\n",
    "# Convert the embedding to a NumPy array\n",
    "prompt_embedding_np = np.array(prompt_embedding).reshape(1, -1)\n",
    "print(\"Prompt embedding generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have the supabase client setup from earlier\n",
    "data = supabase.table('urls_table').select('id', 'url', 'url_embedding').execute()\n",
    "\n",
    "# Extract URLs and embeddings into lists\n",
    "urls = [row['url'] for row in data.data]\n",
    "embeddings = [row['url_embedding'] for row in data.data]\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array for easier similarity calculations\n",
    "import numpy as np\n",
    "url_embeddings_np = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute cosine similarities between the prompt and URL embeddings\n",
    "similarities = cosine_similarity(prompt_embedding_np, url_embeddings_np)\n",
    "\n",
    "# Get the indices of the top 5 most similar URLs\n",
    "top_n = 5\n",
    "similar_indices = np.argsort(similarities[0])[-top_n:][::-1]\n",
    "\n",
    "# Display the most similar URLs as a \"bookshelf\"\n",
    "print(\"Bookshelf/Playlist of Recommended Essays:\")\n",
    "for idx in similar_indices:\n",
    "    print(f\"- {urls[idx]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
